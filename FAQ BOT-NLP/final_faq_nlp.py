# -*- coding: utf-8 -*-
"""final FAQ NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j0gwa-73foPfYdjLqMqscUCfQ5x2viJA
"""

# Step 1: Create directory for FAQ datasets and save the dictionaries

import os
import pickle

# Create directory
if not os.path.exists('dict_texts'):
    os.makedirs('dict_texts')

# Create the FAQ datasets for the three schools
velammal_faq_dict = {
    "What are the admission criteria for Velammal School?": "The admission criteria for Velammal School are based on entrance exam scores and previous academic performance.",
    "What is the school fee structure at Velammal School?": "The school fees are Rs. 50,000 per annum for primary and Rs. 75,000 for secondary.",
    "Does Velammal School offer extracurricular activities?": "Yes, various extracurricular activities including sports, music, and art programs are offered.",
}

rosemary_faq_dict = {
    "What are the admission criteria for Rosemary School?": "Rosemary School admits students based on an interview and a review of previous academic records.",
    "What is the fee structure at Rosemary School?": "The fee structure is Rs. 40,000 per annum for primary and Rs. 65,000 for secondary.",
    "Does Rosemary School offer boarding facilities?": "Yes, boarding facilities are offered for students from Class 6 onwards.",
}

xavier_faq_dict = {
    "What are the school hours at Xavier School?": "The school hours are from 9:00 AM to 4:00 PM, Monday through Friday.",
    "What is the fee structure for Xavier School?": "The fee structure is Rs. 55,000 per annum for primary and Rs. 80,000 for secondary.",
    "Does Xavier School have a library?": "Yes, there is a well-equipped library with a wide range of books.",
}

# Save the datasets as .pkl files
with open('dict_texts/velammal_faq_dict.pkl', 'wb') as f:
    pickle.dump(velammal_faq_dict, f)

with open('dict_texts/rosemary_faq_dict.pkl', 'wb') as f:
    pickle.dump(rosemary_faq_dict, f)

with open('dict_texts/xavier_faq_dict.pkl', 'wb') as f:
    pickle.dump(xavier_faq_dict, f)

print("Datasets created successfully!")

!pip install fuzzywuzzy

# Step 2: Main FAQ Chatbot Code

import pickle
import os
import re
import numpy as np
from fuzzywuzzy import process
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# nltk setup (if you don't have nltk data, download it)
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize text processing tools
tfidf_vectorizer = TfidfVectorizer(lowercase=False)
stop_words = set(stopwords.words('english'))
wordnet_lemmatizer = WordNetLemmatizer()

# Function to load the FAQ data based on user input
def faq_import(school_request):
    school_choices = {
        'Velammal School': 'velammal_faq_dict.pkl',
        'Rosemary School': 'rosemary_faq_dict.pkl',
        'Xavier School': 'xavier_faq_dict.pkl'
    }

    # Fuzzy search to find closest match to user input
    school_match = process.extractOne(school_request, list(school_choices.keys()))

    # Retry if match score is below 50
    while school_match[1] < 50:
        school_request_retry = input('Please respecify the school. You can only query:\nVelammal School // Rosemary School // Xavier School\n')
        school_match = process.extractOne(school_request_retry, list(school_choices.keys()))

    # Select the best match
    school_select = school_match[0]
    school_dict = school_choices[school_select]

    # Load the corresponding FAQ dictionary
    filename = os.path.join('dict_texts', school_dict)
    with open(filename, 'rb') as infile:
        faqs = pickle.load(infile)

    print(f"FAQs for {school_select} loaded successfully.")  # Debug message
    return faqs, school_select

# Text processing function for FAQ questions
def text_process(data):
    only_letters = re.sub("[^a-zA-Z]", " ", data)
    tokens = word_tokenize(only_letters)
    lower_case = [l.lower() for l in tokens]
    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))
    lemmas = [wordnet_lemmatizer.lemmatize(t, pos='v') for t in filtered_result]
    return lemmas

# Function to process user input and FAQ questions
def input_question(question, data, feats):
    if question:
        data.insert(0, question)
    new_feats = text_process(question)
    feats.insert(0, new_feats)
    print(f"Processed question: {question}")  # Debug message
    return data, feats

# Function to calculate cosine similarity between FAQ and user input
def calculate_distances(feats):
    arr = np.array([' '.join(f) for f in feats])
    tfidf_matrix = tfidf_vectorizer.fit_transform(arr)
    distances = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)
    print("Distance matrix calculated.")  # Debug message
    return distances

# Fuzzy search function to find most relevant FAQ
def fuzzy_search(question, faqs):
    fuzzy_faq_matches = process.extract(question, list(faqs.keys()))
    fuzzy_faq_question = fuzzy_faq_matches[0][0]
    fuzzy_faq_score = fuzzy_faq_matches[0][1]
    print(f"Fuzzy search result: {fuzzy_faq_question} with score {fuzzy_faq_score}")  # Debug message
    return fuzzy_faq_question, fuzzy_faq_score

# Function to display the most relevant FAQ response
def similarity_text(distance_matrix, data, fuzzy_question, fuzzy_score, faqs):
    sorted_matrix = np.argsort(distance_matrix[0])[::-1][1:5]
    dist_score1 = distance_matrix[0][sorted_matrix[0]]
    dist_score2 = distance_matrix[0][sorted_matrix[1]]

    print(f"Top similarity score: {dist_score1}")  # Debug message

    if dist_score1 > 0.4 and dist_score1 != dist_score2:
        print('\n' + faqs[data[sorted_matrix[0]]] + '\n')
    elif fuzzy_score > 75:
        print('\n' + faqs[fuzzy_question] + '\n')
    else:
        print("No suitable answer found. Please try to ask again or contact the school.")

# Main function to run the chatbot
def run(question, faqs):
    data = list(faqs.keys())
    feats = [text_process(k) for k in data]
    input_results = input_question(question, data, feats)
    new_data = input_results[0]
    new_feats = input_results[1]
    distance_matrix = calculate_distances(new_feats)
    fuzzy_result = fuzzy_search(question, faqs)
    fuzzy_question, fuzzy_score = fuzzy_result
    similarity_text(distance_matrix, new_data, fuzzy_question, fuzzy_score, faqs)

# Start the chatbot
if __name__ == "__main__":
    school_request = input("Which school would you like to know more about? (Velammal School, Rosemary School, Xavier School)\n")
    faqs, school_select = faq_import(school_request)

    while True:
        question = input(f"What would you like to know about {school_select}? (Type 'quit' to exit)\n")
        if question.lower() == 'quit':
            break
        run(question, faqs)